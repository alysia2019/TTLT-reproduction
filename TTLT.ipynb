{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required packages\n",
    "## Base modules\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Encoding Modules\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check local GPU is running, uncomment and run if using a local GPU\n",
    "#torch.cuda.current_device()\n",
    "#torch.cuda.device(0)\n",
    "#torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll build a helper function to generate simulated reads.\n",
    "## Could've gone for specifying probabilities per base,\n",
    "## but as it's a field test we're keeping it simple.\n",
    "\n",
    "def random_dna_sequence(length):\n",
    "    return np.array(list(''.join(random.choice('ACTG') for _ in range(length))))\n",
    "\n",
    "# Function for generating n x m R matrix,\n",
    "# where n is the number of reads (total_reads)\n",
    "# and m is each read's length (read_length).\n",
    "\n",
    "def create_read_matrix(total_reads,read_length):\n",
    "    \n",
    "    read_matrix = []\n",
    "    \n",
    "    for i in range(total_reads):\n",
    "        read_matrix.append(random_dna_sequence(read_length))\n",
    "        \n",
    "    return np.vstack(read_matrix)\n",
    "\n",
    "#Build a function that simultaneously generates unique, desired length k-mers \n",
    "##from a previously generated read_matrix and times said k-mer occured.\n",
    "## Returns both objects ordered by occurence\n",
    "\n",
    "def create_kmer_matrix(read_matrix,kmer_size):\n",
    "    \n",
    "    kmer_matrix = []\n",
    "    \n",
    "    for i in range(read_matrix.shape[0]):\n",
    "        \n",
    "        for j in range(len(read_matrix[i])-kmer_size+1):\n",
    "            kmer_matrix.append(read_matrix[i][j:j+kmer_size])\n",
    "            \n",
    "    kmer_matrix, counts = np.unique(kmer_matrix,axis=0,return_counts=1)\n",
    "    #inner index to sort by count occurences\n",
    "    sort_index = np.argsort(-counts)\n",
    "    \n",
    "    return kmer_matrix[sort_index], counts[sort_index]\n",
    "\n",
    "## function to turn our kmer matrix into a one hot matrix.\n",
    "\n",
    "def oneshotonehot(kmer_matrix):\n",
    "    \n",
    "    # Not yet robust, breaks when single vector is entered;\n",
    "    # look into reshaping\n",
    "    \n",
    "    #Define bases and fit labels to use\n",
    "    \n",
    "    vectors = kmer_matrix[0]\n",
    "    counts = kmer_matrix[1]\n",
    "    oneshotonehot_matrix = []\n",
    "    bases = ['A','C','T','G']\n",
    "    label_encoder = LabelEncoder()\n",
    "    base_encoder = label_encoder.fit(bases)\n",
    "    \n",
    "    for i in range(len(vectors)):\n",
    "        \n",
    "        encoded_base_seq = base_encoder.transform(vectors[i])\n",
    "        reshaped_vector = encoded_base_seq.reshape(-1)\n",
    "        one_hot_vector = np.eye(len(bases))[reshaped_vector] \n",
    "        oneshotonehot_matrix.append(one_hot_vector)\n",
    "        \n",
    "    return oneshotonehot_matrix, counts\n",
    "\n",
    "## One final wrapper function to generate per-sample data on the spot\n",
    "\n",
    "def generate_kmer_data(total_reads, read_length, kmer_size, number_of_samples):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    sample_num = [] \n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        \n",
    "        read_matrix = create_read_matrix(total_reads,read_length)\n",
    "        kmer_matrix = create_kmer_matrix(read_matrix,kmer_size)\n",
    "        onehot_matrix = oneshotonehot(kmer_matrix)\n",
    "        X.extend(onehot_matrix[0])\n",
    "        Y.extend(onehot_matrix[1])\n",
    "        sample_num.extend([i+1] * len(onehot_matrix[0]))\n",
    "        \n",
    "    return X, Y, sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now generate and load data with our previously created helper functions\n",
    "## X corresponds to one-hot encoded sequence vectors per kmer sequence\n",
    "## Y corresponds to the number of times said kmer was counted\n",
    "## sample corresponds to the sample identifier\n",
    "\n",
    "\n",
    "## Placeholder batch size\n",
    "batch_size = 64\n",
    "\n",
    "X, y, sample = generate_kmer_data(total_reads = 100, read_length = 10 , kmer_size = 5, number_of_samples=2)\n",
    "\n",
    "n_samples = len(X)\n",
    "\n",
    "# We will now split our generated data into train, validation and test sets.\n",
    "# With 80%, 10%, 10% of the complete dataset, respectively\n",
    "## Keeping an additional sample variable with respective identifiers\n",
    "### This might not be the best way to handle tags\n",
    "\n",
    "X_train, y_train, sample_train = (X[:round(0.8*n_samples)], \n",
    "                                  y[:round(0.8*n_samples)], \n",
    "                                  sample[:round(0.8*n_samples)])\n",
    "    \n",
    "X_val, y_val,sample_val = (X[round(0.8*n_samples):round(0.9*n_samples)],\n",
    "                           y[round(0.8*n_samples):round(0.9*n_samples)],\n",
    "                           sample[round(0.8*n_samples):round(0.9*n_samples)])\n",
    "        \n",
    "X_test, y_test,sample_test = (X[round(0.9*n_samples):], \n",
    "                              y[round(0.9*n_samples):], \n",
    "                              sample[round(0.9*n_samples):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now create loader variables for efficient variable management.\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train),torch.FloatTensor(y_train)),\n",
    "                          batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val),torch.FloatTensor(y_val)),\n",
    "                              batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(torch.FloatTensor(X_test),torch.FloatTensor(y_test)),\n",
    "                        batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0049, -0.0548,  0.0254,  ..., -0.0300, -0.0145, -0.0270],\n",
      "        [-0.0332,  0.0069,  0.0244,  ..., -0.0470, -0.0490, -0.0149],\n",
      "        [-0.0185, -0.0592,  0.0281,  ...,  0.0455,  0.0322,  0.0335],\n",
      "        ...,\n",
      "        [-0.0528,  0.0315,  0.0540,  ..., -0.0298,  0.0057, -0.0249],\n",
      "        [ 0.0616, -0.0034,  0.0010,  ...,  0.0202,  0.0032,  0.0015],\n",
      "        [-0.0546, -0.0334,  0.0224,  ..., -0.0281,  0.0227,  0.0377]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0443, -0.0388,  0.0146,  ...,  0.0289, -0.0172,  0.0195],\n",
      "        [ 0.0530, -0.0335,  0.0477,  ...,  0.0423,  0.0264,  0.0391],\n",
      "        [-0.0485, -0.0424, -0.0608,  ...,  0.0345,  0.0427, -0.0034],\n",
      "        ...,\n",
      "        [ 0.0563, -0.0096,  0.0253,  ...,  0.0253,  0.0021, -0.0566],\n",
      "        [-0.0510, -0.0510,  0.0578,  ...,  0.0427,  0.0557, -0.0587],\n",
      "        [ 0.0189,  0.0624, -0.0231,  ..., -0.0164, -0.0038,  0.0182]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Pytorch NN modules\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "\n",
    "## Define optimizer, RMSprop as per TTLT\n",
    "## Model was trained for 200 epochs\n",
    "## Learning rate = 0.001, alpha = 0.99, momentum = 0\n",
    "### alpha and momentum correspond to pytorch's default value\n",
    "### for the torch.optim.RMSprop class; \n",
    "### therefore, we'll only specify learning rate and epochs.\n",
    "\n",
    "class biLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(biLSTM, self).__init__()\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size,\n",
    "                           hidden_size,\n",
    "                           n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "bi_lstm = nn.LSTM(input_size=24, hidden_size = 256 , num_layers = 2, bidirectional=True)\n",
    "reverse_lstm = nn.LSTM(input_size=24, hidden_size = 256 , num_layers = 2, bidirectional=True)\n",
    "\n",
    "#We're going to make weights equal\n",
    "reverse_lstm.weight_ih_l0 = bi_lstm.weight_ih_l0_reverse\n",
    "reverse_lstm.weight_hh_l0 = bi_lstm.weight_hh_l0_reverse\n",
    "reverse_lstm.bias_ih_l0 = bi_lstm.weight_ih_l0_reverse\n",
    "reverse_lstm.bias_hh_l0 = bi_lstm.weight_hh_l0_reverse\n",
    "\n",
    "print(bi_lstm.weight_ih_l0)\n",
    "print(reverse_lstm.weight_ih_l0)\n",
    "#Setting specified loss function, optimizer\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.RMSprop(pass model parameters once built,lr=learning_rate)\n",
    "\n",
    "n_epoch = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Build LSTM\n",
    "## Layers = 2, 256 hidden units per layer.\n",
    "## Embedding output size = 2\n",
    "\n",
    "\n",
    "\n",
    "#bi_output, bi_hidden = bi_lstm()\n",
    "#reverse_output, reverse_hidden = bi_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction layer\n",
    "## MLP\n",
    "## layers = 2, sizes 150 and 100, respectively\n",
    "## Activation function = ReLU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
